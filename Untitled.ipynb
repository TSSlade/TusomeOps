{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\Dropbox\\Berkeley MIDS\\MIDS+\\Type-Token Analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python \"C:\\Dropbox\\TSlade Stata\\Kenya Tusome\\gitrepos\\non-STATA\\Leveled Readers Type-Token Calculation\\Type-Token Calculator.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Would you like to [m]anually enter filenames, or would you prefer to let this run in [a]utomated mode?\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Would you like the routine to [p]rint out the specs for each book as it goes, or would you prefer to let this run [s]ilently?\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Please provide the directory in which your files are stored\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Thank you. Please provide the file extension (format) of the files you intend to process. \n",
      "\te.g. .docx, .txt, .csv, .xlsx\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "This function will run on the following files:\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "[ 1]\tFormat to Use.txt\n",
      "[ 2]\tLR-1-KIS-A-1234-The Empty Story.Slade.txt\n",
      "[ 3]\tLR-4-ENG-D-011A-The Very Hungry Caterpillar.Eric Carle.txt\n",
      "[ 4]\tLR-4-ENG-D-596B-Look at Me.David Waweru.txt\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Please [c]onfirm the list is accurate. If there are any files you want to [r]emove from the list, now is the time.\n",
      "\n",
      "\n",
      "Thank you. We'll proceed. \n",
      "\n",
      "******************************************************************************************************************************\n",
      "The filename Format to Use.txt is malformed; the file will be skipped.\n",
      "\n",
      "\n",
      "******************************************************************************************************************************\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "Analyzing LR-1-KIS-A-1234-The Empty Story.Slade.txt.\n",
      "\n",
      "\n",
      "Analysis of 'The empty story' by Timothy Slade completed.\n",
      "It is expected to be a Level A book with 6 pages and an overall type-token ratio of 1.00\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "Analyzing LR-4-ENG-D-011A-The Very Hungry Caterpillar.Eric Carle.txt.\n",
      "\n",
      "\n",
      "Analysis of 'The very hungry caterpillar' by Eric Carle completed.\n",
      "It is expected to be a Level D book with 14 pages and an overall type-token ratio of 0.47\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "Analyzing LR-4-ENG-D-596B-Look at Me.David Waweru.txt.\n",
      "\n",
      "\n",
      "Analysis of 'Look at me' by David Waweru completed.\n",
      "It is expected to be a Level D book with 8 pages and an overall type-token ratio of 0.41\n",
      "Analysis of the following stories has been completed: \n",
      "[ 1]\tFormat to Use.txt\n",
      "[ 2]\tLR-1-KIS-A-1234-The Empty Story.Slade.txt\n",
      "[ 3]\tLR-4-ENG-D-011A-The Very Hungry Caterpillar.Eric Carle.txt\n",
      "[ 4]\tLR-4-ENG-D-596B-Look at Me.David Waweru.txt\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Would you like to [c]ontinue with a new directory, or\n",
      "would you like to [f]inish and export your analyses?\n",
      "\n",
      "..............................................................................................................................\n",
      "\n",
      "By default, the output will be saved to 'TTR Report 2018-4-28.csv'\n",
      "Please [a]ccept the default or [c]hoose a new filename.\n",
      " <-- end of the filename\n",
      "Your current working directory is \n",
      "\t'C:\\Dropbox\\Berkeley MIDS\\MIDS+\\Type-Token Analysis'\n",
      "By default, the output will be saved to the current working directory.\n",
      "Please [a]ccept the default or [c]hoose a new destination directory.\n",
      "You have supplied 'C:/Dropbox/Berkeley MIDS/MIDS+/Type-Token Analysis/TTR Report 2018-4-28.csv'\n",
      "That directory and filename already exist. To avoid overwriting an existing report the filename will be changed.\n",
      "That directory and filename already exist. To avoid overwriting an existing report the filename will be changed.\n",
      "Our columns will be ['Reader ID', 'Title', 'Author', 'Page Ct', 'Word Ct', 'Type-Token Ratio', 'Types Ct', 'Token Ct', 'Unique Words', 'Words w Counts', 'Avg Length of Words (# Chars)', 'Avg Length of Sentences (# Words)', 'Content']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "dict contains fields not in fieldnames: 'Tokens Ct'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ba6f7e8de286>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;31m# if running_mode == \"m\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[1;31m#     file_list = get_dir_and_file(running_mode)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m     \u001b[0mprocessing_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m         \u001b[1;31m# process_reader(f, verbosity)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ba6f7e8de286>\u001b[0m in \u001b[0;36mprocessing_complete\u001b[1;34m(running_mode)\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnext_task\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"f\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         \u001b[0moutput_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    384\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mnext_task\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"q\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[0mexit_ttr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ba6f7e8de286>\u001b[0m in \u001b[0;36moutput_to_file\u001b[1;34m(dict_of_readers)\u001b[0m\n\u001b[0;32m    464\u001b[0m                             \u001b[1;34m\"Avg Length of Sentences (# Words)\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_len_word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m                             \u001b[1;34m\"Words w Counts\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m                             \"Content\": r.content,})\n\u001b[0m\u001b[0;32m    467\u001b[0m     print(\"Results of your analyses have been written to \\n\\t\" +\n\u001b[0;32m    468\u001b[0m           \"{}\\nPlease review.\".format(full_path))\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\csv.py\u001b[0m in \u001b[0;36mwriterow\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dict_to_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrowdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwriterows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowdicts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\csv.py\u001b[0m in \u001b[0;36m_dict_to_list\u001b[1;34m(self, rowdict)\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwrong_fields\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m                 raise ValueError(\"dict contains fields not in fieldnames: \"\n\u001b[1;32m--> 151\u001b[1;33m                                  + \", \".join([repr(x) for x in wrong_fields]))\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrowdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: dict contains fields not in fieldnames: 'Tokens Ct'"
     ]
    }
   ],
   "source": [
    "# Type-Token Calculator.py\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import csv\n",
    "# import docx\n",
    "\n",
    "\n",
    "reader_dict = {}\n",
    "\n",
    "##############################\n",
    "# Defining custom Exceptions #\n",
    "##############################\n",
    "\n",
    "class BadFilenameException(Exception):\n",
    "    \"\"\" Exception raised when the structure of the filename cannot support the\n",
    "        naming convention required by our program and analysis.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class EmptyFileException(Exception):\n",
    "    \"\"\" Exception raised when the file is empty.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class FileTooShortException(Exception):\n",
    "    \"\"\" Exception raised when the file does not have the required content.\n",
    "        Specifically, the file has too few lines, suggesting it only has headers\n",
    "        and not content.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "###############################################\n",
    "# Defining a class for Leveled Reader objects #\n",
    "###############################################\n",
    "\n",
    "class LeveledReader():\n",
    "    leveled_reader_ct = 0\n",
    "\n",
    "    def __init__(self, id, title, author, page_ct, word_ct,\n",
    "                 tt_ratio, types_ct, token_ct, unique_words, words,\n",
    "                 word_len_char=\"not calculated\",\n",
    "                 sent_len_word=\"N/A\"):\n",
    "        \"\"\"Initialize instance of Leveled Reader book\"\"\"\n",
    "        LeveledReader.leveled_reader_ct += 1\n",
    "        self.id = id\n",
    "        self.title = title\n",
    "        self.author = author\n",
    "        self.page_ct = page_ct\n",
    "        self.word_ct = word_ct\n",
    "        self.tt_ratio = tt_ratio\n",
    "        self.types_ct = types_ct\n",
    "        self.token_ct = token_ct\n",
    "        self.unique_words = unique_words\n",
    "        self.words = words\n",
    "        self.word_len_char = word_len_char\n",
    "        self.sent_len_word = sent_len_word\n",
    "        self.content = {}\n",
    "        for page in range(0, page_ct):\n",
    "            self.content.update(page = \"\")\n",
    "\n",
    "############################################\n",
    "# Functions needed to execute this program #\n",
    "############################################\n",
    "\n",
    "def process_reader(reader_file, verbosity):\n",
    "    \"\"\" Function to create a LeveledReader object from a\n",
    "        file in your filesystem.\n",
    "\n",
    "        Function takes two inputs:\n",
    "            reader_file (str) - name of the file that will be processed;\n",
    "                has the format 'LR-#-{ENG/KIS}-{A-J}-{CODE}-TITLE.AUTHOR'\n",
    "            verbosity (str) - how much output you expect as the function runs\n",
    "\n",
    "    \"\"\"\n",
    "    # Open file, read in contents\n",
    "    fh = open(reader_file, mode=\"r\", encoding=\"UTF-8\")\n",
    "    filename = fh.name.split(\"-\")\n",
    "    if not len(filename) == 6:\n",
    "        print(\"*\" * 126)\n",
    "        print(\"The filename {} is malformed; the file will be skipped.\\n\".format(fh.name))\n",
    "        print()\n",
    "        print(\"*\" * 126)\n",
    "        # raise BadFilenameException\n",
    "        return None\n",
    "    record_id = filename[4] # Uses the unique code generated by Brenda et al\n",
    "    data = fh.readlines()\n",
    "    if len(data) == 0:\n",
    "        print(\"*\" * 126)\n",
    "        print(\"The file {} is empty. Proceeding to the next.\\n\".format(fh.name))\n",
    "        print()\n",
    "        print(\"*\" * 126)\n",
    "        # raise EmptyFileException\n",
    "        return None\n",
    "    elif len(data) < 4:\n",
    "        print(\"*\" * 126)\n",
    "        print(\"The file {} is incomplete. Proceeding to the next.\\n\".format(fh.name))\n",
    "        print()\n",
    "        print(\"*\" * 126)\n",
    "        # raise FileTooShortException\n",
    "        return None\n",
    "    print(\"-\" * 126)\n",
    "    print(\"Analyzing {}.\\n\".format(fh.name))\n",
    "    # Initialize counters, dicts, etc.\n",
    "    curr_line = 0\n",
    "    book_word_ct = 0\n",
    "    book_types_dict = {}\n",
    "    # Define a RegEx substitution pattern to eliminate punctuation we wish to\n",
    "    # ignore\n",
    "    pattern = re.compile(\"[.,:;\\-?!]*\")\n",
    "    # Process the contents of the -reader_file-\n",
    "    for line in data:\n",
    "        line = re.sub(pattern, \"\", line).lower()\n",
    "        words = line.split()\n",
    "        if curr_line == 0:\n",
    "            level_expected = words[1]\n",
    "        elif curr_line == 1:\n",
    "            record_title = \" \".join(words[:]).capitalize()\n",
    "        elif curr_line == 2:\n",
    "            record_author = \" \".join(w.capitalize() for w in words[:])\n",
    "        else:\n",
    "            new_word_ct = 0\n",
    "            new_word_lst = []\n",
    "            page_word_ct = 0\n",
    "            if verbosity == \"p\":\n",
    "                print(\"\\tAs of page {}, new word ct is {}...\".format(curr_line - 2, new_word_ct))\n",
    "            for word in words:\n",
    "                page_word_ct += 1\n",
    "                if word not in book_types_dict.keys():\n",
    "                    new_word_ct += 1\n",
    "                    new_word_lst.append(word)\n",
    "    #             print(word, new_words,\":\",book_types_dict.keys())\n",
    "                book_types_dict[word] = book_types_dict.get(word, 0) + 1\n",
    "            book_word_ct += page_word_ct\n",
    "            book_types_ct = len(book_types_dict.keys())\n",
    "            book_token_ct = sum(book_types_dict.values())\n",
    "            if book_token_ct != 0:\n",
    "                type_token_ratio = book_types_ct/book_token_ct\n",
    "            else:\n",
    "                type_token_ratio = 9999\n",
    "            if verbosity == \"p\":\n",
    "                print(\"\\tPage {}'s {} total words included {} new words: ({})\\n\".format(curr_line - 2, page_word_ct, new_word_ct, new_word_lst) +\n",
    "                  \"\\t\\tTTR is now {:.2f} \".format(type_token_ratio) +\n",
    "                  \"(from {} / {})\".format(book_types_ct, book_token_ct))\n",
    "        curr_line += 1\n",
    "\n",
    "    record_page_ct = curr_line - 1 # B/c incremented at end of loop\n",
    "    unique_words = list(book_types_dict.keys())\n",
    "\n",
    "    # Define the attributes that will be required in order to generate the\n",
    "    # LeveledReader object\n",
    "    record_attribs = [record_id, record_title, record_author, record_page_ct,\n",
    "                      book_word_ct, type_token_ratio, book_types_ct,\n",
    "                      book_token_ct, unique_words, book_types_dict]\n",
    "\n",
    "    # Store the new LeveledReader object in a dict so we can retrieve them all\n",
    "    reader_dict[record_id] = LeveledReader(*record_attribs)\n",
    "    print()\n",
    "    print(\"Analysis of '{}' by {} completed.\".format(record_title, record_author))\n",
    "    print(\"It is expected to be a Level {} book with {} pages and an \".format(level_expected.upper(), curr_line - 3) +\n",
    "          \"overall type-token ratio of {:.2f}\".format(type_token_ratio))\n",
    "    if verbosity == \"p\":\n",
    "        print(\"This is derived from a total word count of {} with the \".format(book_word_ct) +\n",
    "              \"following {} unique words: \".format(book_types_ct))\n",
    "        print()\n",
    "        print(\"+\" * 126)\n",
    "        print()\n",
    "        # for u_word in book_types_dict.keys():\n",
    "            # print(u_word, \", \", end=\"\")\n",
    "        \n",
    "        rows = len(unique_words) // 8\n",
    "        # print(\"Printing out the unique words will require {} rows\".format(rows))\n",
    "        first_word = 0\n",
    "        if rows >= 1:\n",
    "            for r in range(0, rows):\n",
    "                # print(\"Now printing words from {}:{}\".format(first_word, first_word + 8))\n",
    "                words_to_print = unique_words[first_word:(first_word + 8)]\n",
    "                print(\"[{:=2}] \".format(r + 1) +\n",
    "                      \"{:<15}{:<15}\".format(words_to_print[0],words_to_print[1]) +\n",
    "                      \"{:<15}{:<15}\".format(words_to_print[2],words_to_print[3]) +\n",
    "                      \"{:<15}{:<15}\".format(words_to_print[4],words_to_print[5]) +\n",
    "                      \"{:<15}{:<15}\".format(words_to_print[6],words_to_print[7]) +\n",
    "                      \"{:>}\".format(\"+\"))\n",
    "                if r == (rows - 1):\n",
    "                    # first_word += 0\n",
    "                    first_word += 8\n",
    "                    # print(\"On last row {}; first_word is now {}\".format(r, first_word))\n",
    "                else:\n",
    "                    first_word += 8\n",
    "                    # print(\"On row {}; first_word is now {}\".format(r, first_word))\n",
    "            remainder_words = unique_words[first_word:]\n",
    "            print(\"[{:=2}] \".format(r + 2), end=\"\")\n",
    "            for r_word in remainder_words:\n",
    "                print(\"{:<15}\".format(r_word), end=\"\")\n",
    "        else:\n",
    "            print(\"[ 1] \", end=\"\")\n",
    "            for u_word in unique_words:\n",
    "                print(\"{:<15}\".format(u_word), end=\"\")\n",
    "        print(\"\\n\")\n",
    "        print(\"+\" * 126)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def get_dir_and_file(running_mode):\n",
    "    \"\"\" Function to get input from the user\"\"\"\n",
    "    print(\"-\" * 126)\n",
    "    print()\n",
    "    print(\"Please provide the directory in which your files are stored\")\n",
    "    lr_dir = input(\"\\t >>: \")\n",
    "    print()\n",
    "    exit_ttr() if lr_dir == \"q\" else lr_dir\n",
    "    backslash = re.compile(r\"\\\\\")\n",
    "    lr_dir = re.sub(backslash, \"/\", lr_dir)\n",
    "    os.chdir(lr_dir)\n",
    "    file_list = []          # Will need to move this outside of the command in order to have a persistent file list...\n",
    "    if running_mode == \"m\":\n",
    "        print(\"-\" * 126)\n",
    "        print()\n",
    "        print(\"Which file would you like to process at this time?:\")\n",
    "        f = input(\"\\t >>: \")\n",
    "        print()\n",
    "        # print(os.path.join(lr_dir, f), \"<-- The file to be processed...\")\n",
    "        file_list.append(f) if os.path.isfile(os.path.join(lr_dir, f)) else file_list\n",
    "    elif running_mode == \"a\":\n",
    "        print(\"-\" * 126)\n",
    "        print()\n",
    "        print(\"Thank you. Please provide the file extension (format) of the \" +\n",
    "              \"files you intend to process. \\n\\te.g. .docx, .txt, .csv, .xlsx\")\n",
    "        lr_ftype = input(\"\\t >>: \")\n",
    "        print()\n",
    "        exit_ttr() if lr_ftype == \"q\" else lr_ftype\n",
    "        file_list = [f for f in os.listdir(lr_dir) if os.path.isfile(\n",
    "                                                    os.path.join(lr_dir, f))\n",
    "                                                    and f.endswith(lr_ftype)]\n",
    "    print(\"-\" * 126)\n",
    "    print()\n",
    "    print(\"This function will run on the following files:\")\n",
    "    confirmation, file_list = verify_file_list(file_list)\n",
    "    print()\n",
    "    exit_ttr() if confirmation == \"q\" else confirmation\n",
    "    if confirmation == \"c\":\n",
    "        print(\"Thank you. We'll proceed. \\n\")\n",
    "    return file_list\n",
    "\n",
    "#############################\n",
    "# User Interaction Function #\n",
    "#############################\n",
    "def user_interaction():\n",
    "    \"\"\" Function to govern user interaction, including obtaining the\n",
    "        preferred -running_mode- and -verbosity-, and calling the\n",
    "        -get_dir_and_file- function\n",
    "    \"\"\"\n",
    "    print(\"-\" * 126)\n",
    "    print()\n",
    "    print(\"Would you like to [m]anually enter filenames, or \" +\n",
    "          \"would you prefer to let this run in [a]utomated mode?\")\n",
    "    running_mode = input(\"\\t >>: \").lower()\n",
    "    print()\n",
    "    while running_mode not in \"maq\":\n",
    "        running_mode = input_nudge(\"user\")\n",
    "        print(\".\" * 126)\n",
    "        print(\"\\n\")\n",
    "    if running_mode == \"q\":\n",
    "        print(\"Thank you for using Type-Token Calculator. Good-bye.\")\n",
    "        sys.exit()\n",
    "    print(\"-\" * 126)\n",
    "    print()\n",
    "    print(\"Would you like the routine to [p]rint out the specs for each book as \" +\n",
    "          \"it goes, or would you prefer to let this run [s]ilently?\")\n",
    "    verbosity = input(\"\\t >>: \").lower()\n",
    "    print()\n",
    "    while verbosity not in \"psq\":\n",
    "        verbosity = input_nudge(\"verbosity\")\n",
    "        print(\".\" * 126)\n",
    "        print(\"\\n\")\n",
    "    if verbosity == \"q\":\n",
    "        print(\"Thank you for using Type-Token Calculator. Good-bye.\")\n",
    "        sys.exit()\n",
    "    return running_mode, verbosity\n",
    "\n",
    "def verify_file_list(file_list):\n",
    "    \"\"\" Function to nicely print out the current file list through which\n",
    "        the -process_reader- function will loop.\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"~\" * 126)\n",
    "    print_file_list(file_list)\n",
    "    print(\"Please [c]onfirm the list is accurate. If there are any files you \" +\n",
    "            \"want to [r]emove from the list, now is the time.\")\n",
    "    confirmation = input(\"\\t >>: \").lower()\n",
    "    print()\n",
    "    while confirmation not in \"rc\":\n",
    "        confirmation = input_nudge(\"verify\")\n",
    "        print()\n",
    "    if confirmation == \"r\":\n",
    "        file_to_drop = \"\"\n",
    "        while file_to_drop is not \"f\":\n",
    "            print(\"-\" * 126)\n",
    "            print()\n",
    "            print(\"Please provide the number for the file you wish to remove.\\n\" +\n",
    "                  \"When you have [f]inished, please enter 'f'.\")\n",
    "            file_to_drop = input(\"\\t >>: \")\n",
    "            print()\n",
    "            if re.match(\"[0-9]+\", file_to_drop):\n",
    "                removed = file_list.pop(int(file_to_drop) - 1)\n",
    "                print(\"{} removed.\".format(removed))\n",
    "                print_file_list(file_list)\n",
    "        if file_to_drop == \"f\":\n",
    "            confirmation = \"c\"\n",
    "    return confirmation, file_list\n",
    "\n",
    "def print_file_list(file_list):\n",
    "    \"\"\" Function to nicely print out the current file list through which\n",
    "        the -process_reader- function will loop.\n",
    "    \"\"\"\n",
    "    file_num = 0\n",
    "    for f in file_list:\n",
    "        file_num += 1\n",
    "        print(\"[{:>2}]\\t{}\".format(file_num, f))\n",
    "    print(\"~\" * 126)\n",
    "    print()\n",
    "\n",
    "def exit_ttr():\n",
    "    \"\"\"Function to gracefully exit the Type-Token Ratio calculator\n",
    "    \"\"\"\n",
    "    print(\"Thank you for using Type-Token Calculator. Good-bye.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "def input_nudge(sending_routine):\n",
    "    \"\"\" Routine to remind the user of their input options.\n",
    "    \"\"\"\n",
    "    print(\".\" * 126)\n",
    "    print(\"Sorry, that input was not understood. To recap, valid inputs are\")\n",
    "    if sending_routine == \"proc_comp\":\n",
    "        print(\"\\t[c] for 'continue' (new story or directory)\\n\" +\n",
    "              \"\\t[f] for 'finish' (export results and close out of the \" +\n",
    "              \"Type-Token Ratio Calculator\")\n",
    "    elif sending_routine == \"verify\":\n",
    "        print(\"\\t[c] for 'confirm' (that all files listed are to be analyzed)\\n\" +\n",
    "              \"\\t[r] for 'remove' (to remove a file from the list provided)\")\n",
    "    elif sending_routine == \"user\":\n",
    "        print(\"\\t[m] for 'manual' mode (provide--and analyze--files one at \" +\n",
    "              \"a time)\\n\" +\n",
    "              \"\\t[a] for 'automatic' mode (provide a directory containing a \" +\n",
    "              \"batch of files to analyze)\")\n",
    "    elif sending_routine == \"verbosity\":\n",
    "        print(\"\\t[p] for 'print' mode (reports updated figures for each page \" +\n",
    "              \"during the analysis)\\n\" +\n",
    "              \"\\t[s] for 'silent' mode (reports only the TTR at the end of \" +\n",
    "              \"each analysis)\")\n",
    "    elif sending_routine == \"output_fname\":\n",
    "        print(\"\\t[a] to 'accept' the default filename\\n\" +\n",
    "              \"\\t[c] to 'choose' a different filename\")\n",
    "    elif sending_routine == \"output_dname\":\n",
    "        print(\"\\t[a] to 'accept' the current output directory\\n\" +\n",
    "              \"\\t[c] to 'choose' a different output directory\")\n",
    "    print(\"\\t[q] for 'quit' (to quit the Type-Token Ratio Calculator without \" +\n",
    "          \"writing the results to file)\")\n",
    "    new_command = input(\"\\t >>: \").lower()\n",
    "    exit_ttr() if new_command == \"q\" else new_command\n",
    "    return new_command\n",
    "\n",
    "def processing_complete(running_mode):\n",
    "    \"\"\" Routine to run when file processing has completed.\n",
    "    \"\"\"\n",
    "    print(\"Analysis of the following stories has been completed: \")\n",
    "    print_file_list(file_list)\n",
    "    # if running_mode == \"m\":\n",
    "    #     prompt = \"story\"\n",
    "    # elif running_mode == \"a\":\n",
    "    #     prompt = \"directory\"\n",
    "    prompt = \"story\" if (running_mode == \"m\") else \"directory\" #if (running_mode == \"a\") else \"\"\n",
    "    print(\"Would you like to [c]ontinue with a new {}, or\\n\".format(prompt) +\n",
    "          \"would you like to [f]inish and export your analyses?\")\n",
    "    next_task = input(\"\\t >>: \").lower()\n",
    "    print()\n",
    "    while next_task not in \"cfq\":\n",
    "        input_nudge(\"proc_comp\")\n",
    "        next_task = input(\"\\t >>: \").lower()\n",
    "        print()\n",
    "    if next_task == \"f\":\n",
    "        output_to_file(reader_dict)\n",
    "    elif next_task == \"q\":\n",
    "        exit_ttr()\n",
    "\n",
    "def output_to_file(dict_of_readers):\n",
    "    \"\"\" Function to output the contents of the dict containing the leveled\n",
    "        reader analysis to a .csv file\n",
    "\n",
    "        Takes as input a dictionary rile\n",
    "\n",
    "    Arguments:\n",
    "        dict_of_readers {dict} -- k: reader IDs, v: LeveledReader Objects\n",
    "    \"\"\"\n",
    "    print(\".\" * 126)\n",
    "    print()\n",
    "    date = datetime.datetime.now()\n",
    "    date_str = \"{}-{}-{}\".format(date.year, date.month, date.day)\n",
    "    destination_fname = \"TTR Report {}.csv\".format(date_str)\n",
    "    print(\"By default, the output will be saved to '{}'\".format(destination_fname))\n",
    "    print(\"Please [a]ccept the default or [c]hoose a new filename.\")\n",
    "    accept_default_fname = input(\"\\t >>: \").lower()\n",
    "    while accept_default_fname not in \"ac\":\n",
    "        accept_default_fname = input_nudge(\"output_fname\")\n",
    "    if accept_default_fname == \"c\":\n",
    "        destination_fname = input(\"\\t >>: \")\n",
    "    print(destination_fname[-1:-5],\"<-- end of the filename\")\n",
    "    destination_fname += \".csv\" if (destination_fname[-4:] != \".csv\") else \"\"\n",
    "    destination_dir = os.getcwd()\n",
    "    print(\"Your current working directory is \\n\\t'{}'\".format(destination_dir))\n",
    "    print(\"By default, the output will be saved to the current working directory.\")\n",
    "    print(\"Please [a]ccept the default or [c]hoose a new destination directory.\")\n",
    "    accept_default_destination = input(\"\\t >>: \")\n",
    "    while accept_default_destination not in \"ac\":\n",
    "        accept_default_destination = input_nudge(\"output_dname\")\n",
    "    if accept_default_destination == \"c\":\n",
    "        print(\"Please provide the directory in which you would like your report.\")\n",
    "        destination_dir = input(\"\\t >>: \")\n",
    "        while not os.exists(destination_dir):\n",
    "            print(\"Sorry, that directory is not found.\")\n",
    "            destination_dir = input(\"\\t >>: \")\n",
    "    full_path = os.path.join(destination_dir, destination_fname)\n",
    "    backslash = re.compile(r\"\\\\\")\n",
    "    full_path = re.sub(backslash, \"/\", full_path)\n",
    "    print(\"You have supplied '{}'\".format(full_path))\n",
    "    while os.path.isfile(full_path):\n",
    "        print(\"That directory and filename already exist. To avoid overwriting \" +\n",
    "              \"an existing report the filename will be changed.\")\n",
    "        increment = 0\n",
    "        full_path = full_path[:-4] + \".\" + str(increment) + \".csv\"\n",
    "        increment += 1\n",
    "    # try:\n",
    "    with open(full_path, \"w\", newline=\"\") as csvfile:\n",
    "        lro_attribs = list(vars(dict_of_readers[list(dict_of_readers.keys())[0]]).keys())\n",
    "        remapping_dict = {'id': \"Reader ID\",\n",
    "                          'title': \"Title\",\n",
    "                          'author': \"Author\",\n",
    "                          'page_ct': \"Page Ct\",\n",
    "                          'word_ct': \"Word Ct\",\n",
    "                          'types_ct': \"Types Ct\",\n",
    "                          'token_ct': \"Token Ct\",\n",
    "                          'word_len_char': \"Avg Length of Words (# Chars)\",\n",
    "                          'sent_len_word': \"Avg Length of Sentences (# Words)\",\n",
    "                          'content': \"Content\",\n",
    "                          'words': \"Words w Counts\",\n",
    "                          'unique_words': \"Unique Words\",\n",
    "                          'tt_ratio': \"Type-Token Ratio\"}\n",
    "        columns = [remapping_dict[a] for a in lro_attribs]\n",
    "        print(\"Our columns will be {}\".format(columns))\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "        writer.writeheader()\n",
    "        for r in dict_of_readers.values():\n",
    "            writer.writerow({\"Reader ID\": r.id,\n",
    "                            \"Title\": r.title,\n",
    "                            \"Author\": r.author,\n",
    "                            \"Type-Token Ratio\": r.tt_ratio,\n",
    "                            \"Types Ct\": r.types_ct,\n",
    "                            \"Tokens Ct\": r.token_ct,\n",
    "                            \"Unique Words\": r.unique_words,\n",
    "                            \"Word Ct\": r.word_ct,\n",
    "                            \"Page Ct\": r.page_ct,\n",
    "                            \"Avg Length of Words (# Chars)\": r.word_len_char,\n",
    "                            \"Avg Length of Sentences (# Words)\": r.sent_len_word,\n",
    "                            \"Words w Counts\": r.words,\n",
    "                            \"Content\": r.content,})\n",
    "    print(\"Results of your analyses have been written to \\n\\t\" +\n",
    "          \"{}\\nPlease review.\".format(full_path))\n",
    "    # except:\n",
    "    #     print(\"Sorry, that directory and filename will not work. Let's try again.\")\n",
    "    #     output_to_file(dict_of_readers)\n",
    "    # except:\n",
    "        # exit_ttr()\n",
    "\n",
    "\n",
    "#########################\n",
    "# Top-Level Interaction #\n",
    "#########################\n",
    "while True:\n",
    "    running_mode, verbosity = user_interaction()\n",
    "    file_list = get_dir_and_file(running_mode)\n",
    "    for f in file_list:\n",
    "        process_reader(f, verbosity)\n",
    "    # if running_mode == \"m\":\n",
    "    #     file_list = get_dir_and_file(running_mode)\n",
    "    processing_complete(running_mode)\n",
    "        # process_reader(f, verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-6-27aac2dde3ed>\u001b[0m(440)\u001b[0;36moutput_to_file\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    438 \u001b[1;33m                                \u001b[1;34m\"Avg Length of Sentences (# Words)\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_len_word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m    439 \u001b[1;33m                                \u001b[1;34m\"Words w Counts\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 440 \u001b[1;33m                                \"Content\": r.content,})\n",
      "\u001b[0m\u001b[1;32m    441 \u001b[1;33m        print(\"Results of your analyses have been written to \\n\\t\" +\n",
      "\u001b[0m\u001b[1;32m    442 \u001b[1;33m              \"{}\\nPlease review.\".format(full_path))\n",
      "\u001b[0m\n",
      "{'1234': <__main__.LeveledReader object at 0x0000022728470F28>, '011A': <__main__.LeveledReader object at 0x0000022728470E10>, '596B': <__main__.LeveledReader object at 0x0000022728470EF0>}\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "C:\\Dropbox\\Berkeley MIDS\\MIDS+\\Type-Token Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
